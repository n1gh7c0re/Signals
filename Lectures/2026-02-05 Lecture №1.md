
### Конспект лекции №1  
**Обработка и интерпретация сигналов**  
**Тема: Введение в канальное кодирование. Модель системы связи, двоичный симметричный канал, основные понятия теории кодов**

**Цель курса**  
Изучение теории передачи данных через каналы с помехами и способов канального кодирования, позволяющих добавлять избыточность для борьбы с ошибками.

### 1. Упрощённая модель цифровой системы связи

```
Источник данных  
    ↓ (информационные символы)  
Кодер канала  
    ↓ (кодовые символы с избыточностью)  
Канал с шумом  
    ↓ (кодовые символы + ошибки)  
Декодер  
    ↓ (оценка информационных символов)  
Получатель
```

- **Источник данных** — генерирует информационные символы (обычно биты 0/1).  
- **Кодер канала** — добавляет избыточность → кодовые символы.  
- **Канал** — вносит ошибки.  
- **Декодер** — пытается восстановить исходные информационные символы.

### 2. Математическая модель канала  
Работаем с **дискретными последовательностями** над полем GF(2).

**Двоичный симметричный канал (ДСК / BSC)**

```
Вход → [Канал] → Выход
  0  ──(1−p)──→  0
  0  ────p────→  1
  1  ────p────→  0
  1  ──(1−p)──→  1
```

- **p** — вероятность ошибки одного символа (переходная вероятность).  
- Пример: p = 10⁻³ — на 1000 бит в среднем 1 ошибка.

### 3. Примеры канальных кодов

#### 3.1. Код с тройным повторением (3,1)-код

| Информационный бит | Кодовое слово |
|--------------------|---------------|
| 0                  | 000           |
| 1                  | 111           |

- **Декодирование** — мажоритарное (по большинству).  
- Исправляет **t = 1** ошибку.  
- Скорость кода **R = 1/3**.

#### 3.2. Линейный (5,2)-код

| Информационные биты | Кодовое слово |
|---------------------|---------------|
| 00                  | 00000         |
| 01                  | 01101         |
| 10                  | 10110         |
| 11                  | 11011         |

- Также исправляет **t = 1** ошибку.  
- Скорость **R = 2/5 = 0.4** (> 1/3) → **экономичнее** предыдущего.

### 4. Основные характеристики кода

- **N** — длина кодового слова  
- **K** — число информационных символов  
- **Скорость кода** R = K/N (0 < R ≤ 1)
- **$d_{min}$** — минимальное расстояние Хэмминга кода

### 5. Теорема Шеннона (1948)

**Пропускная способность ДСК:**

$$C = 1 - h_2(p), \quad h_2(p) = -p\log_2 p - (1-p)\log_2(1-p)$$

**Утверждение:**  
Если R < C, то существуют коды (при достаточно больших N), обеспечивающие **сколь угодно малую** вероятность ошибки декодирования.  
Если R > C — надёжная передача невозможна.

**Пример:** p = 10⁻³ → C ≈ 0.988  
Достаточно всего **~2% избыточности**, чтобы теоретически добиться почти идеальной надёжности.

### 6. Метрики Хэмминга

- **Вес Хэмминга** ω(x) — число единиц в векторе x.  
- **Расстояние Хэмминга** d(x,y) = ω(x ⊕ y) (для GF(2)).  
- **Минимальное расстояние кода** $d_{min}$ = min{d(x,y) | x ≠ y ∈ C}

**Корректирующая способность:**

$t \leq \lfloor (d-1)/2 \rfloor$, где $\lfloor x \rfloor$ - наибольшее целое, не превышающее $x$

### 7. Линейные коды

Код линейный, если сумма (по mod 2) любых двух кодовых слов тоже принадлежит коду.

Для линейных кодов:  
$d_{min}$ = минимальный ненулевой вес кодовых слов.

**Порождающая матрица G** (размер K × N)  
Строки — базисные векторы.  
Кодовое слово: c = m · G

**Проверочная матрица H** (размер (N−K) × N)  
c · Hᵀ = 0 ∀ c ∈ C
